#!/usr/bin/env python3
"""
åŸºäºLangChainçš„å¢å¼ºç‰ˆTopic-RAGç³»ç»Ÿ
"""

import os
import sys
import torch
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional
from datetime import datetime

# LangChainå¯¼å…¥
from langchain_core.language_models import LLM
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import LLMChain, SequentialChain
from langchain.memory import ConversationBufferMemory
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Import models
# æ·»åŠ æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if root_dir not in sys.path:
    sys.path.insert(0, root_dir)

sys.path.append(os.path.join(root_dir, 'src'))
sys.path.append(os.path.join(root_dir, 'rag', 'utils'))

try:
    from src.topic_model import MultiOmicsETM
    from load_separate_models import load_separate_models
    print("âœ… Successfully imported load_separate_models from root directory")

    try:
        from deepseek_translate import deepseek_translate
        print("âœ… Successfully imported deepseek_translate")
    except ImportError:
        try:
            # å°è¯•ä»rag.utilså¯¼å…¥
            from rag.utils.deepseek_translate import deepseek_translate
            print("âœ… Successfully imported deepseek_translate from rag.utils")
        except ImportError:
            print("âš ï¸ deepseek_translate not available, using fallback")
            def deepseek_translate(text, target_lang="en", api_key=None):
                return text

except ImportError as e:
    print(f"âŒ Model import failed: {e}")
    print(f"Current working directory: {os.getcwd()}")
    print(f"Python path: {sys.path}")

    # Create a simple placeholder function
    def load_separate_models(model_dir):
        print("Warning: Using placeholder model loading function")
        return None, None, None, None
    def deepseek_translate(text, target_lang="en", api_key=None):
        print("Warning: Translation function not available")
        return text

class DeepSeekLangChainLLM(LLM):
    """DeepSeek LLMçš„LangChainåŒ…è£…å™¨"""
    
    api_key: str
    model_name: str = "deepseek-chat"
    base_url: str = "https://api.deepseek.com/v1"
    
    def __init__(self, api_key: str, model_name: str = "deepseek-chat"):
        super().__init__(api_key=api_key, model_name=model_name)
        
    @property
    def _llm_type(self) -> str:
        return "deepseek"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """è°ƒç”¨DeepSeek API"""
        import requests
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 1500,
            "temperature": 0.7
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json()["choices"][0]["message"]["content"]
            else:
                return f"APIè°ƒç”¨å¤±è´¥: {response.status_code}"
                
        except Exception as e:
            return f"è°ƒç”¨å¤±è´¥: {str(e)}"

import base64
import io
from PIL import Image
from openai import OpenAI

class LangChainTopicRAGSystem:
    """åŸºäºLangChainçš„å®Œæ•´Topic-RAGç³»ç»Ÿ"""
    
    def __init__(self, model_dir='/root/autodl-tmp/AETM/models', device='cpu', api_key=None, doubao_api_key=None):
        """
        Initialize complete LangChain Topic-RAG system
        """
        print(f"ğŸš€ Initializing LangChain Topic-RAG System...")
        print(f"   Model directory: {model_dir}")
        print(f"   Device: {device}")

        self.device = device
        self.model_dir = model_dir
        self.api_key = api_key
        self.doubao_api_key = doubao_api_key

        # Load models and components
        print("ğŸ“Š Loading complete models and components...")
        try:
            self.model, self.vectorizer, self.vocab, self.theta = load_separate_models(model_dir)
            print("âœ… Model components loaded successfully")
        except Exception as e:
            print(f"âŒ Model loading failed: {e}")
            raise

        if self.model is None:
            raise Exception("Model loading failed, please check if model files exist")

        self.model.to(device)
        self.model.eval()

        # Load original data - using absolute path
        data_path = '/root/autodl-tmp/AETM/data/palettes_descriptions.xlsx'
        print(f"ğŸ“š Loading data file: {data_path}")

        if not os.path.exists(data_path):
            print(f"âŒ Data file does not exist: {data_path}")
            # Try to list directory contents
            data_dir = os.path.dirname(data_path)
            if os.path.exists(data_dir):
                print(f"Data directory contents: {os.listdir(data_dir)}")
            raise FileNotFoundError(f"Data file does not exist: {data_path}")

        try:
            self.df_plans = pd.read_excel(data_path)
            print(f"âœ… Data file loaded successfully, {len(self.df_plans)} records")
        except Exception as e:
            print(f"âŒ Data file reading failed: {e}")
            raise

        # Initialize LangChain components
        print("ğŸ”§ Initializing LangChain components...")
        self._init_langchain_components()

        # Build complete retrieval database
        print("ğŸ—ƒï¸ Building complete retrieval database...")
        self._build_complete_retrieval_database()

        print("âœ… Complete LangChain Topic-RAG system initialization completed!")
    
    def _translate_text(self, text: str, target_lang: str = "en") -> str:
        """Translate text using DeepSeek API"""
        try:
            if not self.api_key:
                print("Warning: No API key provided, translation skipped")
                return text

            translated = deepseek_translate(text, target_lang=target_lang, api_key=self.api_key)
            return translated
        except Exception as e:
            print(f"Translation failed: {e}")
            return text

    def _detect_language(self, text: str) -> str:
        """Simple language detection"""
        # Count Chinese characters
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        total_chars = len(text.replace(' ', ''))

        if total_chars == 0:
            return "en"

        chinese_ratio = chinese_chars / total_chars
        return "zh" if chinese_ratio > 0.3 else "en"

    def _get_fewshot_examples(self):
        """è·å–Few-ShotèŒƒä¾‹ï¼Œç”¨äºæ•™ä¼šLLMæŒ‰æŒ‡å®šæ•°é‡ç”Ÿæˆé¢œè‰²"""
        examples = {
            "3_colors": {
                "input": "I need an extremely minimalist color scheme for a coffee brand logo, only 3 colors needed.",
                "output": """### **Minimalist Coffee Logo**
**Design Concept**: Convey warmth and professionalism through ultimate simplicity. Inspired by a fresh latte, using minimal colors to express core concepts: coffee, milk, and focus.
---
### **Color Scheme**
1. **Espresso Black - RGB(50, 40, 35)**
   *Represents the coffee base, stable and professional.*
2. **Warm Milk White - RGB(245, 240, 235)**
   *Represents blended milk, warm and pure.*
3. **Terracotta Brown - RGB(180, 110, 80)**
   *Represents the coffee cup, adding handcrafted warmth and texture.*"""
            },
            "7_colors": {
                "input": "Design a 7-color environmental palette for a fantasy RPG game set in a magical forest.",
                "output": """### **Magical Forest "Elven Twilight"**
**Design Concept**: Capture moonlight filtering through ancient forest canopy, illuminating magical creatures and plants. Create an atmosphere that's both serene and filled with potential danger and wonder.
---
### **Color Scheme**
1. **Midnight Blue - RGB(20, 25, 55)**
   *Forest's night backdrop, vast and mysterious.*
2. **Ancient Wood Brown - RGB(60, 45, 40)**
   *Massive tree trunks, full of power.*
3. **Moss Green - RGB(80, 110, 70)**
   *Moss covering roots and stones, representing life and moisture.*
4. **Moonlight Silver - RGB(210, 220, 230)**
   *Cold moonlight piercing through leaves, main light source.*
5. **Ghost Mushroom Cyan - RGB(100, 200, 180)**
   *Glowing magical mushrooms, fantasy elements and accent lights.*
6. **Fairy Blood Red - RGB(190, 50, 60)**
   *Hidden magical flowers or warning colors of dangerous creatures.*
7. **Earth Gray - RGB(100, 95, 90)**
   *Forest rocks and soil, providing neutral transitions.*"""
            }
        }
        return examples

    def _create_fewshot_prompt(self, num_colors: int, user_query: str, retrieved_context: str = "") -> str:
        """ä½¿ç”¨Few-Shotæ–¹æ³•åˆ›å»ºæç¤ºè¯ï¼Œç¡®ä¿ç”ŸæˆæŒ‡å®šæ•°é‡çš„é¢œè‰²"""
        examples = self._get_fewshot_examples()

        # é€‰æ‹©åˆé€‚çš„èŒƒä¾‹
        if num_colors <= 4:
            primary_example = examples["3_colors"]
            secondary_example = examples["7_colors"]
        else:
            primary_example = examples["7_colors"]
            secondary_example = examples["3_colors"]

        prompt = f"""# Role
You are a top-tier color design master who can precisely generate the specified number of colors according to user requirements and provide complete design solutions.

# Task
Please learn from the following examples to understand how to construct your answer based on the specified number of colors. Then, create a brand new color scheme based on the final user's real requirements.

---
[Example 1: Input]
User Requirements: "{primary_example['input']}"
Reference Cases: (omitted or empty)

[Example 1: Output]
{primary_example['output']}

---
[Example 2: Input]
User Requirements: "{secondary_example['input']}"
Reference Cases: (omitted or empty)

[Example 2: Output]
{secondary_example['output']}

---

# User's Real Requirements and Reference Cases
Now, please strictly follow the format of the above examples and generate a brand new color scheme containing **{num_colors}** colors based on the following user's real requirements.

[Real Requirements: Input]
User Requirements: "{user_query}"
Reference Cases: "{retrieved_context}"

[Real Requirements: Output]
"""

        return prompt

    def _init_langchain_components(self):
        """Initialize LangChain components"""
        if not self.api_key:
            print("Warning: No API key provided, LangChain functionality will be limited")
            # Set default values to avoid attribute errors
            self.llm = None
            self.memory = None
            self.rag_generation_chain = None
            return

        # Initialize LLM
        self.llm = DeepSeekLangChainLLM(self.api_key)

        # Initialize memory
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )

        # Create Prompt templates
        self._create_prompt_templates()

        # Create processing chains
        self._create_processing_chains()

        print("âœ… LangChain components initialized successfully")
    
    def _create_prompt_templates(self):
        """Create Prompt templates"""

        # Image understanding template
        self.image_understanding_template = PromptTemplate(
            input_variables=["image_path"],
            template="""Please analyze the design features of image {image_path}, including:
1. Overall design style (modern, classical, minimalist, luxurious, etc.)
2. Main color combinations and tones
3. Design elements and layout characteristics
4. Applicable scenarios and space types
5. Emotional atmosphere created

Please describe in concise and professional language, focusing on color combinations."""
        )

        # Text fusion template
        self.text_fusion_template = PromptTemplate(
            input_variables=["user_text", "image_analysis"],
            template="""Please generate a detailed design scheme description based on the following information:

User Requirements: {user_text}
Image Analysis: {image_analysis}

Please integrate user requirements and image analysis to generate a detailed design scheme description, including:
1. Overall design style (combining user requirements and image characteristics)
2. Color matching suggestions (based on image colors and user preferences)
3. Design element characteristics (integrating image elements and user requirements)
4. Applicable scenarios (clearly specify the usage environment)
5. Emotional atmosphere (describe the specific emotions created by the design)

Requirements: Professional language, specific descriptions, highlighting color combinations, reflecting personalization."""
        )

        # RAG generation template
        self.rag_generation_template = PromptTemplate(
            input_variables=["prompt"],
            template="""You are a professional design aesthetics expert. Please generate a brand new, personalized design scheme based on the following information:

{prompt}

Please use the above reference schemes as inspiration, combined with user requirements, to generate a completely new design scheme. Requirements:

1. **Design Style**: Create a new style different from the reference schemes, clearly explain the innovation points
2. **Color Matching**: Generate 5 brand new RGB color values that are significantly different from the reference scheme colors
3. **Design Elements**: Describe innovative design elements and layouts, reflecting originality
4. **Applicable Scenarios**: Expand or change the application scenarios of the reference schemes
5. **Emotional Atmosphere**: Create an emotional atmosphere different from the reference schemes

**Important Requirements**:
- The generated RGB color values must be significantly different from the colors in the reference schemes
- The design style should be different from the reference schemes
- Ensure it is a brand new creative work, not a simple modification of the reference schemes
- Color combinations should have logic and aesthetic value

Please answer in English with professional and elegant language, highlighting innovation and personalization."""
        )
    
    def _create_processing_chains(self):
        """åˆ›å»ºå¤„ç†é“¾"""
        
        # ç®€åŒ–ä¸ºå•ä¸ªRAGç”Ÿæˆé“¾
        self.rag_generation_chain = LLMChain(
            llm=self.llm,
            prompt=self.rag_generation_template,
            memory=self.memory
        )
    
    def _build_complete_retrieval_database(self):
        """æ„å»ºå®Œæ•´çš„æ£€ç´¢æ•°æ®åº“ï¼ˆåŒ…å«æ–‡æœ¬å’Œé¢œè‰²å‘é‡ï¼‰"""
        print("æ­£åœ¨æ„å»ºå®Œæ•´æ£€ç´¢æ•°æ®åº“...")
        
        # ä½¿ç”¨è®­ç»ƒå¥½çš„thetaçŸ©é˜µä½œä¸ºä¸»é¢˜è¡¨ç¤º
        self.theta_vectors = self.theta.cpu().numpy()
        
        # æ„å»ºé¢œè‰²å‘é‡
        color_cols = [f'Color_{i}_{c}' for i in range(1, 6) for c in ['R', 'G', 'B']]
        self.color_vectors = self.df_plans[color_cols].values
        
        # å­˜å‚¨åŸå§‹æ–‡æœ¬æè¿°
        self.text_descriptions = self.df_plans['description'].values
        
        # ç¡®ä¿æ•°ç»„é•¿åº¦ä¸€è‡´
        min_length = min(len(self.theta_vectors), len(self.color_vectors), len(self.text_descriptions))
        
        # æˆªå–åˆ°æœ€å°é•¿åº¦
        self.theta_vectors = self.theta_vectors[:min_length]
        self.color_vectors = self.color_vectors[:min_length]
        self.text_descriptions = self.text_descriptions[:min_length]
        
        # é¢„è®¡ç®—å½’ä¸€åŒ–å‘é‡ç”¨äºå¿«é€Ÿæ£€ç´¢
        from sklearn.preprocessing import normalize
        self.theta_vectors_normalized = normalize(self.theta_vectors, norm='l2', axis=1)
        self.color_vectors_normalized = normalize(self.color_vectors, norm='l2', axis=1)
        
        print(f"å®Œæ•´æ£€ç´¢æ•°æ®åº“æ„å»ºå®Œæˆï¼š{len(self.text_descriptions)}ä¸ªæ–‡æ¡£")
    
    def _complete_topic_model_inference(self, fused_text: str) -> tuple:
        """å®Œæ•´çš„Topic Modelæ¨ç†ï¼ˆæ¨¡å—ä¸€ï¼‰"""
        try:
            # æ–‡æœ¬å‘é‡åŒ–
            text_bow = self._text_to_bow(fused_text)
            
            # Topic Modelæ¨ç†
            self.model.eval()
            with torch.no_grad():
                # æ–‡æœ¬ç¼–ç 
                text_tensor = torch.FloatTensor(text_bow).to(self.device)
                mu_text, logvar_text = self.model.encode_text(text_tensor)
                
                # é¢œè‰²ç¼–ç ï¼ˆä½¿ç”¨é›¶å‘é‡ä½œä¸ºå ä½ç¬¦ï¼‰
                mu_color = torch.zeros_like(mu_text)
                logvar_color = torch.ones_like(logvar_text) * 10
                
                # å¤šæ¨¡æ€èåˆ
                mu, logvar = self.model.product_of_gaussians(mu_color, logvar_color, mu_text, logvar_text)
                theta = self.model.get_theta(mu)
                
                # è§£ç é‡æ„
                recon_color, recon_text = self.model.decode(theta)
                
                return (
                    recon_text.cpu().numpy(),
                    recon_color.cpu().numpy(),
                    theta.cpu().numpy(),
                    mu.cpu().numpy(),
                    logvar.cpu().numpy()
                )
                
        except Exception as e:
            print(f"Topic Modelæ¨ç†å¤±è´¥: {e}")
            return None, None, None, None, None

    def _dual_stage_retrieval(self, theta_query: np.ndarray, recon_color: np.ndarray, top_k: int = 10) -> List[Dict]:
        """åŒé˜¶æ®µæ£€ç´¢ï¼šæ–‡æœ¬ç›¸ä¼¼åº¦ + é¢œè‰²ç›¸ä¼¼åº¦"""
        from sklearn.metrics.pairwise import cosine_similarity
        from sklearn.preprocessing import normalize
        
        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šåŸºäºä¸»é¢˜å‘é‡çš„æ–‡æœ¬ç›¸ä¼¼åº¦æ£€ç´¢
            theta_query_norm = normalize(theta_query, norm='l2', axis=1)
            text_similarities = cosine_similarity(theta_query_norm, self.theta_vectors_normalized).flatten()
            
            # è·å–æ–‡æœ¬Top-Kå€™é€‰ï¼ˆæ‰©å¤§å€™é€‰æ± ï¼‰
            text_top_k = min(top_k * 3, len(text_similarities))
            text_top_indices = np.argsort(text_similarities)[-text_top_k:][::-1]
            
            # ç¬¬äºŒé˜¶æ®µï¼šåœ¨å€™é€‰ä¸­è¿›è¡Œé¢œè‰²ç›¸ä¼¼åº¦é‡æ’åº
            candidate_results = []
            recon_color_norm = normalize(recon_color, norm='l2', axis=1)
            
            for idx in text_top_indices:
                # è®¡ç®—é¢œè‰²ç›¸ä¼¼åº¦
                candidate_color_vec = self.color_vectors_normalized[idx:idx+1]
                color_similarity = cosine_similarity(recon_color_norm, candidate_color_vec)[0][0]
                
                # ç»¼åˆå¾—åˆ†ï¼ˆå¯è°ƒæ•´æƒé‡ï¼‰
                text_score = text_similarities[idx]
                color_score = color_similarity
                combined_score = 0.6 * text_score + 0.4 * color_score
                
                candidate_results.append({
                    'index': idx,
                    'text_score': text_score,
                    'color_score': color_score,
                    'combined_score': combined_score,
                    'description': self.text_descriptions[idx],
                    'colors': self.color_vectors[idx].reshape(-1, 3)
                })
            
            # æŒ‰ç»¼åˆå¾—åˆ†æ’åºï¼Œè¿”å›Top-K
            candidate_results.sort(key=lambda x: x['combined_score'], reverse=True)
            return candidate_results[:top_k]
            
        except Exception as e:
            print(f"åŒé˜¶æ®µæ£€ç´¢å¤±è´¥: {e}")
            return []

    def _text_to_bow(self, text: str) -> np.ndarray:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯è¢‹å‘é‡"""
        try:
            if self.vectorizer is None:
                raise Exception("TF-IDFå‘é‡åŒ–å™¨æœªåŠ è½½")
            
            # ä½¿ç”¨è®­ç»ƒå¥½çš„TF-IDFå‘é‡åŒ–å™¨
            bow_vector = self.vectorizer.transform([text]).toarray().astype(np.float32)
            return bow_vector
            
        except Exception as e:
            print(f"æ–‡æœ¬å‘é‡åŒ–å¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
            vocab_size = len(self.vocab) if self.vocab else 2000
            return np.zeros((1, vocab_size), dtype=np.float32)

    def _retrieve_candidates_langchain(self, user_text: str, top_k: int = 10) -> List[Dict]:
        """ä½¿ç”¨LangChainæ–¹æ³•æ£€ç´¢å€™é€‰æ–¹æ¡ˆ"""
        try:
            # ä½¿ç”¨å®Œæ•´çš„Topic Modelæ¨ç†
            recon_text, recon_color, theta_query, mu, logvar = self._complete_topic_model_inference(user_text)

            if theta_query is None:
                print("Topic Modelæ¨ç†å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ£€ç´¢")
                return self._simple_text_retrieval(user_text, top_k)

            # ä½¿ç”¨åŒé˜¶æ®µæ£€ç´¢
            candidates = self._dual_stage_retrieval(theta_query, recon_color, top_k)
            return candidates

        except Exception as e:
            print(f"LangChainæ£€ç´¢å¤±è´¥: {e}")
            return self._simple_text_retrieval(user_text, top_k)

    def _simple_text_retrieval(self, user_text: str, top_k: int = 10) -> List[Dict]:
        """ç®€å•çš„æ–‡æœ¬æ£€ç´¢ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity

            # åˆ›å»ºç®€å•çš„TF-IDFå‘é‡
            texts = [user_text] + list(self.text_descriptions)
            vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
            tfidf_matrix = vectorizer.fit_transform(texts)

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()

            # è·å–Top-K
            top_indices = np.argsort(similarities)[-top_k:][::-1]

            candidates = []
            for idx in top_indices:
                candidates.append({
                    'index': idx,
                    'text_score': similarities[idx],
                    'color_score': 0.5,  # é»˜è®¤é¢œè‰²å¾—åˆ†
                    'combined_score': similarities[idx] * 0.8 + 0.5 * 0.2,
                    'description': self.text_descriptions[idx],
                    'colors': self.color_vectors[idx].reshape(-1, 3)
                })

            return candidates

        except Exception as e:
            print(f"ç®€å•æ£€ç´¢ä¹Ÿå¤±è´¥: {e}")
            return []

    def run_langchain_pipeline(self, user_text: str, image_path: str = None, top_k: int = 5, num_colors: int = 5) -> Dict[str, Any]:
        """Run LangChain enhanced RAG pipeline with Few-Shot color control"""

        print("ğŸš€ Starting LangChain Topic-RAG System...")

        # Step 0: Detect language and translate if needed
        original_lang = self._detect_language(user_text)
        print(f"Detected language: {original_lang}")

        # Translate user input to English for processing
        if original_lang == "zh":
            user_text_en = self._translate_text(user_text, target_lang="en")
            print("Translated user input to English for processing")
        else:
            user_text_en = user_text

        # Step 1: Retrieve candidate schemes
        print("Step 1: Retrieving candidate schemes...")
        candidates = self._retrieve_candidates_langchain(user_text_en, top_k)

        # Build reference text in English
        reference_text = "Reference Design Schemes (for inspiration only, do not copy):\n\n"
        for i, candidate in enumerate(candidates[:3], 1):
            colors = candidate['colors']
            color_desc = ""
            for j, color in enumerate(colors, 1):
                color_desc += f"Color {j}: RGB({int(color[0]*255)}, {int(color[1]*255)}, {int(color[2]*255)})\n"

            reference_text += f"Scheme {i}:\nDescription: {candidate['description']}\nColors:\n{color_desc}\n"

        # Step 2: Generate new scheme using LLM
        print("Step 2: Generating new scheme using LLM...")

        try:
            # Check if LLM is available
            if self.llm is None:
                print("LLM not available, generating simple color scheme")
                return self._generate_simple_plan(candidates, user_text_en, original_lang)

            # Build Few-Shot prompt with specified number of colors
            full_prompt = self._create_fewshot_prompt(num_colors, user_text_en, reference_text)

            # Call LLM directly
            generated_plan = self.llm._call(full_prompt)
            print(f"Generated scheme length: {len(generated_plan)}")

            return {
                'generated_plan': generated_plan,
                'candidates': candidates[:3],
                'original_language': original_lang,
                'user_text_original': user_text if original_lang == "zh" else None
            }

        except Exception as e:
            print(f"LangChain processing failed: {e}")
            return self._generate_simple_plan(candidates, user_text_en, original_lang)
    
    def _generate_simple_plan(self, candidates: List[Dict], user_text: str, original_lang: str = "en") -> Dict[str, Any]:
        """Generate simple color scheme (when LLM is not available)"""
        try:
            if not candidates:
                return {
                    'error': 'No suitable reference schemes found',
                    'candidates': []
                }

            # Generate simple scheme based on best candidate
            best_candidate = candidates[0]
            colors = best_candidate['colors']

            # Generate color descriptions
            color_descriptions = []
            color_names = ["Primary Color", "Secondary Color", "Accent Color", "Background Color", "Emphasis Color"]

            for i, color in enumerate(colors[:5]):
                r = int(color[0] * 255)
                g = int(color[1] * 255)
                b = int(color[2] * 255)
                color_name = color_names[i] if i < len(color_names) else f"Color {i+1}"
                color_descriptions.append(f"{color_name}: RGB({r}, {g}, {b})")

            # Generate simple scheme description in English
            simple_plan = f"""**Design Concept**
Based on your requirements "{user_text}", we recommend a carefully matched color scheme. This scheme integrates modern design concepts, maintaining visual harmony while highlighting key elements.

**Color Scheme**
{chr(10).join(color_descriptions)}

**Application Suggestions**
This color scheme is suitable for the application scenarios you mentioned. We recommend using the primary color for large areas, secondary color for functional areas, accent color for highlighting important elements, background color for maintaining spatial cleanliness, and emphasis color for guiding key information.

**Design Features**
- Rich color layers with excellent visual effects
- Complies with modern aesthetic trends
- Balance of practicality and aesthetics
- Easy to apply and match in practice"""

            return {
                'generated_plan': simple_plan,
                'candidates': candidates[:3],
                'note': 'Using simplified generation mode (LLM not available)',
                'original_language': original_lang
            }

        except Exception as e:
            return {
                'error': f'Simple scheme generation failed: {str(e)}',
                'candidates': candidates[:3] if candidates else []
            }

    def get_conversation_history(self) -> List[str]:
        """è·å–å¯¹è¯å†å²"""
        return self.memory.chat_memory.messages if self.memory else []

    def clear_memory(self):
        """æ¸…é™¤è®°å¿†"""
        if self.memory:
            self.memory.clear()

    def _analyze_image_with_doubao(self, image_path: str) -> str:
        """ä½¿ç”¨è±†åŒ…APIåˆ†æå›¾ç‰‡"""
        try:
            if not self.doubao_api_key:
                return "æœªé…ç½®è±†åŒ…APIå¯†é’¥ï¼Œä½¿ç”¨é»˜è®¤å›¾ç‰‡æè¿°"

            from openai import OpenAI
            from PIL import Image
            import base64
            import io

            # è¯»å–å¹¶å¤„ç†å›¾ç‰‡
            image = Image.open(image_path)

            # è½¬æ¢ä¸ºRGBæ ¼å¼
            if image.mode != 'RGB':
                image = image.convert('RGB')

            # è°ƒæ•´å›¾ç‰‡å¤§å°
            max_size = (800, 600)
            if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
                image.thumbnail(max_size, Image.Resampling.LANCZOS)

            # ç¼–ç å›¾ç‰‡
            img_buffer = io.BytesIO()
            image.save(img_buffer, format='JPEG', quality=85, optimize=True)
            img_buffer.seek(0)
            image_data = img_buffer.getvalue()
            image_base64 = base64.b64encode(image_data).decode('utf-8')

            # è®¾ç½®ç¯å¢ƒå˜é‡
            os.environ["ARK_API_KEY"] = self.doubao_api_key

            # åˆå§‹åŒ–è±†åŒ…å®¢æˆ·ç«¯
            client = OpenAI(
                base_url="https://ark.cn-beijing.volces.com/api/v3",
                api_key=self.doubao_api_key,
            )

            # è°ƒç”¨è±†åŒ…è§†è§‰æ¨¡å‹
            response = client.chat.completions.create(
                model="doubao-1-5-thinking-vision-pro-250428",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}"
                                },
                            },
                            {
                                "type": "text",
                                "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š1.ä¸»è¦ç‰©ä½“å’Œåœºæ™¯ 2.è‰²å½©æ­é…å’Œé£æ ¼ç‰¹ç‚¹ 3.æ•´ä½“æ°›å›´å’Œæ„Ÿå— 4.å¯èƒ½çš„è®¾è®¡é£æ ¼ã€‚è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€æè¿°ï¼Œé‡ç‚¹å…³æ³¨è‰²å½©å’Œè®¾è®¡ç›¸å…³çš„ä¿¡æ¯ã€‚"
                            },
                        ],
                    }
                ],
                timeout=30
            )

            return response.choices[0].message.content

        except Exception as e:
            print(f"è±†åŒ…å›¾ç‰‡åˆ†æå¤±è´¥: {e}")
            return f"å›¾ç‰‡åˆ†æå¤±è´¥: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤æè¿°ï¼šç°ä»£è®¾è®¡é£æ ¼ï¼Œè‰²å½©æ­é…å’Œè°"

    def run_complete_pipeline(self, user_text: str, image_path: str = None, top_k: int = 5, num_colors: int = 5) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„ä¸¤æ¨¡å—RAGæµç¨‹ï¼ˆä¸ºStreamlitåº”ç”¨æä¾›çš„æ¥å£ï¼‰"""
        try:
            print("ğŸš€ å¯åŠ¨å®Œæ•´çš„ä¸¤æ¨¡å—RAGæµç¨‹...")

            # æ¨¡å—ä¸€ï¼šå›¾ç‰‡ç†è§£ + æ–‡æœ¬èåˆ + Topic Modelæ¨ç†
            print("ğŸ“Š æ¨¡å—ä¸€ï¼šå›¾ç‰‡ç†è§£ä¸æ–‡æœ¬èåˆ...")

            # å›¾ç‰‡ç†è§£ï¼ˆå¦‚æœæä¾›äº†å›¾ç‰‡ï¼‰
            image_analysis = ""
            if image_path and os.path.exists(image_path):
                try:
                    # ä½¿ç”¨è±†åŒ…APIè¿›è¡Œå›¾ç‰‡ç†è§£
                    image_analysis = self._analyze_image_with_doubao(image_path)
                    print("âœ… å›¾ç‰‡åˆ†æå®Œæˆ")
                except Exception as e:
                    print(f"å›¾ç‰‡åˆ†æå¤±è´¥: {e}")
                    image_analysis = "å›¾ç‰‡åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æè¿°ï¼šç°ä»£ç®€çº¦è®¾è®¡é£æ ¼ï¼Œè‰²å½©æ­é…å’Œè°"

            # æ–‡æœ¬èåˆ
            if image_analysis:
                fused_text = f"ç”¨æˆ·éœ€æ±‚: {user_text}\nå›¾ç‰‡åˆ†æ: {image_analysis}"
            else:
                fused_text = user_text

            print("ğŸ“ˆ Topic Modelæ¨ç†...")

            # Topic Modelæ¨ç†
            recon_text, recon_color, theta_query, mu, logvar = self._complete_topic_model_inference(fused_text)

            if theta_query is None:
                return {
                    'error': 'Topic Modelæ¨ç†å¤±è´¥',
                    'fused_text': fused_text,
                    'image_analysis': image_analysis
                }

            # æ¨¡å—äºŒï¼šæ£€ç´¢ + é‡æ’åº + ç”Ÿæˆ
            print("ğŸ” æ¨¡å—äºŒï¼šæ£€ç´¢ä¸ç”Ÿæˆ...")

            # åŒé˜¶æ®µæ£€ç´¢
            candidates = self._dual_stage_retrieval(theta_query, recon_color, top_k)

            if not candidates:
                return {
                    'error': 'æ£€ç´¢å¤±è´¥ï¼Œæœªæ‰¾åˆ°ç›¸å…³å€™é€‰æ–¹æ¡ˆ',
                    'fused_text': fused_text,
                    'image_analysis': image_analysis
                }

            # ä½¿ç”¨LangChainç”Ÿæˆæœ€ç»ˆæ–¹æ¡ˆï¼Œä¼ å…¥é¢œè‰²æ•°é‡å‚æ•°
            result = self.run_langchain_pipeline(user_text, image_path, top_k, num_colors)

            # æ·»åŠ å¤„ç†è¿‡ç¨‹ä¿¡æ¯
            result['fused_text'] = fused_text
            result['image_analysis'] = image_analysis
            result['module_one_success'] = True
            result['module_two_success'] = True

            return result

        except Exception as e:
            print(f"å®Œæ•´æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'error': f'å®Œæ•´æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}',
                'fused_text': user_text,
                'image_analysis': '',
                'module_one_success': False,
                'module_two_success': False
            }

def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºLangChain RAGç³»ç»Ÿ"""
    
    api_key = "sk-3c4ba59c8b094106995821395c7bc60e"
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    system = LangChainTopicRAGSystem(device='cpu', api_key=api_key)
    
    # æµ‹è¯•ç”¨ä¾‹
    user_text = "æˆ‘æƒ³è¦ä¸€ä¸ªç°ä»£ç®€çº¦é£æ ¼çš„é…è‰²æ–¹æ¡ˆï¼Œé€‚åˆåŠå…¬ç¯å¢ƒ"
    image_path = "test_image.jpg"
    
    # è¿è¡ŒLangChainå¢å¼ºçš„RAGæµç¨‹
    result = system.run_langchain_pipeline(user_text, image_path)
    
    if 'error' not in result:
        print("\n" + "="*60)
        print("âœ¨ LangChainç”Ÿæˆçš„æ–¹æ¡ˆ âœ¨")
        print("="*60)
        print(result['generated_plan'])
        print("="*60)
        
        print(f"\nğŸ“Š APIè°ƒç”¨ç»Ÿè®¡:")
        print(f"æ€»Tokenæ•°: {result['api_stats']['total_tokens']}")
        print(f"æ€»æˆæœ¬: ${result['api_stats']['total_cost']:.4f}")
        
        print(f"\nğŸ“ å›¾ç‰‡åˆ†æ:")
        print(result['image_analysis'])
        
        print(f"\nğŸ”„ èåˆæ–‡æœ¬:")
        print(result['fused_text'][:200] + "...")
    else:
        print(f"âŒ å¤„ç†å¤±è´¥: {result['error']}")

if __name__ == "__main__":
    main() 
