#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ„å»ºå¹¶ä¿å­˜çŸ¥è¯†åº“
åŸºäºTopic Modelè®¡ç®—æ‰€æœ‰è¯åº“æ¡ç›®çš„å‘é‡è¡¨ç¤º
"""

import os
import sys
import json
import pandas as pd
import numpy as np
import torch
import pickle
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/root/autodl-tmp/AETM')
from load_separate_models import load_separate_models

class KnowledgeBaseBuilder:
    """çŸ¥è¯†åº“æ„å»ºå™¨"""
    
    def __init__(self, model_dir='../../models', data_path='../../data/palettes_descriptions.xlsx', device='cpu'):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“æ„å»ºå™¨
        
        Args:
            model_dir: æ¨¡å‹æ–‡ä»¶ç›®å½•
            data_path: è¯åº“æ•°æ®è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device
        self.model_dir = model_dir
        self.data_path = data_path
        
        print("ğŸš€ åˆå§‹åŒ–çŸ¥è¯†åº“æ„å»ºå™¨...")
        
        # 1. åŠ è½½topic model
        print("ğŸ“Š åŠ è½½Topic Model...")
        self.model, self.vectorizer, self.vocab, self.theta = load_separate_models(model_dir)
        self.model.to(device)
        self.model.eval()
        print(f"   æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model.num_topics}ä¸ªä¸»é¢˜, {len(self.vocab)}ä¸ªè¯æ±‡")
        
        # 2. åŠ è½½è¯åº“æ•°æ®
        print("ğŸ“š åŠ è½½è¯åº“æ•°æ®...")
        self.df_knowledge = pd.read_excel(data_path)
        print(f"   è¯åº“å¤§å°: {len(self.df_knowledge)} æ¡è®°å½•")
        
        print("âœ… çŸ¥è¯†åº“æ„å»ºå™¨åˆå§‹åŒ–å®Œæˆï¼")
    
    def _text_to_bow(self, text: str):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºBOWå‘é‡"""
        try:
            bow_vector = self.vectorizer.transform([text])
            return bow_vector.toarray()[0]
        except Exception as e:
            print(f"   BOWè½¬æ¢å¤±è´¥: {e}")
            return np.zeros(len(self.vocab))
    
    def _topic_model_inference(self, bow_vector):
        """ä½¿ç”¨topic modelè¿›è¡Œæ¨ç†"""
        try:
            with torch.no_grad():
                bow_tensor = torch.FloatTensor(bow_vector).unsqueeze(0).to(self.device)
                
                # åˆ›å»ºé›¶é¢œè‰²å‘é‡
                color_tensor = torch.zeros(1, self.model.color_dim).to(self.device)
                
                # ç¼–ç 
                mu_color, logvar_color = self.model.encode_color(color_tensor)
                mu_text, logvar_text = self.model.encode_text(bow_tensor)
                
                # é«˜æ–¯ä¹˜ç§¯
                mu, logvar = self.model.product_of_gaussians(mu_color, logvar_color, mu_text, logvar_text)
                
                # é‡å‚æ•°åŒ–
                delta = self.model.reparameterize(mu, logvar)
                theta = self.model.get_theta(delta)
                
                # è§£ç 
                recon_color, recon_text = self.model.decode(theta)
                
                return recon_text.cpu().numpy(), recon_color.cpu().numpy()
                
        except Exception as e:
            print(f"   Topic modelæ¨ç†å¤±è´¥: {e}")
            return np.zeros((1, len(self.vocab))), np.zeros((1, 15))
    
    def build_knowledge_base(self):
        """æ„å»ºçŸ¥è¯†åº“"""
        print("ğŸ” å¼€å§‹æ„å»ºçŸ¥è¯†åº“...")
        
        # æå–æè¿°æ–‡æœ¬
        descriptions = self.df_knowledge['description'].fillna('').tolist()
        names = self.df_knowledge['names'].fillna('').astype(str).tolist()
        
        # ä¸ºæ¯ä¸ªæè¿°è®¡ç®—topicå‘é‡å’Œé¢œè‰²å‘é‡
        knowledge_text_vectors = []
        knowledge_color_vectors = []
        knowledge_colors_rgb = []
        
        print(f"   æ­£åœ¨ä¸º {len(descriptions)} ä¸ªè¯åº“æ¡ç›®è®¡ç®—Topic Modelå‘é‡...")
        
        for i, desc in enumerate(descriptions):
            try:
                # å°†æè¿°è½¬æ¢ä¸ºBOW
                desc_bow = self._text_to_bow(desc)
                
                # é€šè¿‡topic modelæ¨ç†
                with torch.no_grad():
                    recon_text_prob, recon_color_prob = self._topic_model_inference(desc_bow)
                    
                    # ç›´æ¥ä½¿ç”¨é‡æ„çš„æ–‡æœ¬æ¦‚ç‡ä½œä¸ºæ–‡æœ¬è¡¨ç¤º
                    knowledge_text_vectors.append(recon_text_prob.flatten())
                    knowledge_color_vectors.append(recon_color_prob.flatten())
                
                # æå–RGBé¢œè‰²
                colors_rgb = []
                for j in range(1, 6):
                    r = self.df_knowledge.iloc[i][f'Color_{j}_R']
                    g = self.df_knowledge.iloc[i][f'Color_{j}_G'] 
                    b = self.df_knowledge.iloc[i][f'Color_{j}_B']
                    colors_rgb.append([float(r), float(g), float(b)])
                knowledge_colors_rgb.append(colors_rgb)
                
                if (i + 1) % 500 == 0:
                    print(f"   å·²å¤„ç† {i + 1}/{len(descriptions)} æ¡è®°å½• ({(i+1)/len(descriptions)*100:.1f}%)")
                    
            except Exception as e:
                print(f"   è­¦å‘Šï¼šç¬¬{i}æ¡è®°å½•å¤„ç†å¤±è´¥: {e}")
                # ä½¿ç”¨é›¶å‘é‡ä½œä¸ºå ä½ç¬¦
                knowledge_text_vectors.append(np.zeros(len(self.vocab)))
                knowledge_color_vectors.append(np.zeros(15))
                knowledge_colors_rgb.append([[0, 0, 0]] * 5)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        knowledge_text_vectors = np.array(knowledge_text_vectors)
        knowledge_color_vectors = np.array(knowledge_color_vectors)
        
        print(f"   çŸ¥è¯†åº“æ„å»ºå®Œæˆ:")
        print(f"   - æ–‡æœ¬å‘é‡: {knowledge_text_vectors.shape}")
        print(f"   - é¢œè‰²å‘é‡: {knowledge_color_vectors.shape}")
        print(f"   - RGBé¢œè‰²: {len(knowledge_colors_rgb)} æ¡è®°å½•")
        
        # æ„å»ºçŸ¥è¯†åº“å­—å…¸
        knowledge_base = {
            'metadata': {
                'total_entries': len(descriptions),
                'text_vector_shape': knowledge_text_vectors.shape,
                'color_vector_shape': knowledge_color_vectors.shape,
                'vocab_size': len(self.vocab),
                'num_topics': self.model.num_topics,
                'color_dim': self.model.color_dim,
                'build_time': datetime.now().isoformat(),
                'model_dir': self.model_dir,
                'data_path': self.data_path
            },
            'data': {
                'descriptions': descriptions,
                'names': names,
                'knowledge_text_vectors': knowledge_text_vectors,
                'knowledge_color_vectors': knowledge_color_vectors,
                'knowledge_colors_rgb': knowledge_colors_rgb
            }
        }
        
        return knowledge_base
    
    def save_knowledge_base(self, knowledge_base, save_path='knowledge_base.pkl'):
        """ä¿å­˜çŸ¥è¯†åº“åˆ°æ–‡ä»¶"""
        print(f"ğŸ’¾ ä¿å­˜çŸ¥è¯†åº“åˆ°: {save_path}")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)
        
        # ä¿å­˜ä¸ºpickleæ–‡ä»¶
        with open(save_path, 'wb') as f:
            pickle.dump(knowledge_base, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(save_path) / (1024 * 1024)  # MB
        print(f"âœ… çŸ¥è¯†åº“ä¿å­˜æˆåŠŸï¼æ–‡ä»¶å¤§å°: {file_size:.1f} MB")
        
        # ä¿å­˜å…ƒæ•°æ®ä¸ºJSONï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
        metadata_path = save_path.replace('.pkl', '_metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(knowledge_base['metadata'], f, ensure_ascii=False, indent=2)
        print(f"ğŸ“‹ å…ƒæ•°æ®ä¿å­˜åˆ°: {metadata_path}")
    
    def build_and_save(self, save_path='knowledge_base.pkl'):
        """æ„å»ºå¹¶ä¿å­˜çŸ¥è¯†åº“"""
        print("ğŸš€ å¼€å§‹æ„å»ºå¹¶ä¿å­˜çŸ¥è¯†åº“...")
        
        # æ„å»ºçŸ¥è¯†åº“
        knowledge_base = self.build_knowledge_base()
        
        # ä¿å­˜çŸ¥è¯†åº“
        self.save_knowledge_base(knowledge_base, save_path)
        
        print("ğŸ‰ çŸ¥è¯†åº“æ„å»ºå’Œä¿å­˜å®Œæˆï¼")
        return knowledge_base

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='æ„å»ºå¹¶ä¿å­˜çŸ¥è¯†åº“')
    parser.add_argument('--model_dir', type=str, default='../../models', help='æ¨¡å‹ç›®å½•')
    parser.add_argument('--data_path', type=str, default='../../data/palettes_descriptions.xlsx', help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='knowledge_base.pkl', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--device', type=str, default='cpu', help='è®¡ç®—è®¾å¤‡')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹æ„å»ºçŸ¥è¯†åº“")
    print(f"æ¨¡å‹ç›®å½•: {args.model_dir}")
    print(f"æ•°æ®è·¯å¾„: {args.data_path}")
    print(f"è¾“å‡ºè·¯å¾„: {args.output}")
    print(f"è®¡ç®—è®¾å¤‡: {args.device}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_path}")
        return
    
    # æ„å»ºçŸ¥è¯†åº“
    try:
        builder = KnowledgeBaseBuilder(
            model_dir=args.model_dir,
            data_path=args.data_path,
            device=args.device
        )
        
        knowledge_base = builder.build_and_save(args.output)
        
        print(f"\nğŸ“Š æ„å»ºç»Ÿè®¡:")
        print(f"æ€»æ¡ç›®æ•°: {knowledge_base['metadata']['total_entries']}")
        print(f"æ–‡æœ¬å‘é‡ç»´åº¦: {knowledge_base['metadata']['text_vector_shape']}")
        print(f"é¢œè‰²å‘é‡ç»´åº¦: {knowledge_base['metadata']['color_vector_shape']}")
        print(f"è¯æ±‡è¡¨å¤§å°: {knowledge_base['metadata']['vocab_size']}")
        print(f"ä¸»é¢˜æ•°é‡: {knowledge_base['metadata']['num_topics']}")
        
    except Exception as e:
        print(f"âŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
